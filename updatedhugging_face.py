# -*- coding: utf-8 -*-
"""updatedhugging face.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kL47ydnI9ToVW1w5TdgyGccltH1EJuIU
"""

from transformers import pipeline

sentiment_pipeline = pipeline("sentiment-analysis", framework="tf")

result = sentiment_pipeline(["I love my college!",'I hate people opinion about AI'])
print(result)

from transformers import BertTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-multilingual-cased")

text = "I love learning about NLP!"
tokens = tokenizer(text, return_tensors="tf", padding=True, truncation=True)

print("Tokenized Words:", tokenizer.tokenize(text))
print("Token IDs:", tokens["input_ids"].numpy().tolist()[0])

outputs = model(**tokens)

logits = outputs.logits
probs = tf.nn.softmax(logits, axis=-1)

label_idx = tf.argmax(probs, axis=-1).numpy()[0]
labels = ["NEGATIVE", "POSITIVE"]
print(f"Predicted Sentiment: {labels[label_idx]}, Probability: {probs.numpy().max():.4f}")

generator = pipeline("text-generation", model="gpt2")

prompt = "There once lived a king"
result = generator(prompt, max_length=30, num_return_sequences=1)

print(result)

qa_pipeline = pipeline("question-answering")

context = """The Great Wall of China is a historic fortification that stretches
over 13,000 miles. It was primarily built to protect against invasions and
was constructed during the Ming Dynasty."""

question = "Who built the Great Wall of China?"

result = qa_pipeline(question=question, context=context)

print(result)